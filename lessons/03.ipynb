{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3주차: 오디오 파일 처리와 변환\n",
    "\n",
    "## 학습 목표\n",
    "- 오디오를 데이터로 변환하는 전체 파이프라인을 이해하자\n",
    "- librosa로 오디오 파일을 읽고 처리하는 방법을 익히자\n",
    "- 시간 도메인과 주파수 도메인 변환을 수행하자\n",
    "- 오디오 특징을 추출하고 분석해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 디지털 오디오의 기초\n",
    "\n",
    "디지털 오디오는 연속적인 아날로그 신호를 이산적인 디지털 데이터로 변환한 것이다. 이 과정에서 샘플링과 양자화가 일어난다.\n",
    "\n",
    "샘플링 레이트(Sample Rate)는 1초당 측정하는 샘플의 개수를 의미한다. CD 품질은 44,100Hz(44.1kHz)를 사용한다.\n",
    "비트 깊이(Bit Depth)는 각 샘플의 진폭을 표현하는 비트 수다. 16비트나 24비트가 일반적이다.\n",
    "오디오 파일 형식에는 무손실(WAV, FLAC)과 손실(MP3, AAC) 압축 방식이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 시각화 설정\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "print(f\"Librosa 버전: {librosa.__version__}\")\n",
    "print(\"오디오 처리 라이브러리 로딩 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 테스트 오디오 신호 생성\n",
    "\n",
    "### 2.1 기본 오디오 신호 생성\n",
    "\n",
    "실제 오디오 파일을 다루기 전에, 수학적으로 정의된 신호를 생성하여 개념을 이해해보자.\n",
    "사인파, 코사인파, 그리고 이들의 조합으로 복잡한 신호를 만들 수 있다.\n",
    "이러한 기본 신호들은 푸리에 변환의 기초가 되며, 모든 복잡한 소리를 구성하는 기본 요소다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플링 파라미터 설정\n",
    "sample_rate = 22050  # 22.05 kHz (librosa 기본값)\n",
    "duration = 3.0  # 3초\n",
    "n_samples = int(sample_rate * duration)\n",
    "\n",
    "# 시간 배열 생성\n",
    "t = np.linspace(0, duration, n_samples, endpoint=False)\n",
    "\n",
    "# 단일 주파수 신호 생성 (A4 음, 440Hz)\n",
    "frequency_a4 = 440.0\n",
    "amplitude = 0.5\n",
    "sine_wave = amplitude * np.sin(2 * np.pi * frequency_a4 * t)\n",
    "\n",
    "print(f\"샘플링 레이트: {sample_rate} Hz\")\n",
    "print(f\"신호 길이: {duration} 초\")\n",
    "print(f\"총 샘플 수: {n_samples}\")\n",
    "print(f\"신호 형태: {sine_wave.shape}\")\n",
    "print(f\"신호 범위: [{sine_wave.min():.3f}, {sine_wave.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 복합 신호 생성 (화음)\n",
    "\n",
    "실제 음악은 여러 주파수가 합쳐진 복합 신호다.\n",
    "화음은 여러 음이 동시에 울릴 때 생성되며, 각 음의 주파수 비율이 화음의 특성을 결정한다.\n",
    "메이저 코드, 마이너 코드 등 다양한 화음을 수학적으로 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C 메이저 코드 생성 (C, E, G)\n",
    "c4_freq = 261.63  # C4\n",
    "e4_freq = 329.63  # E4\n",
    "g4_freq = 392.00  # G4\n",
    "\n",
    "# 각 음 생성\n",
    "c_note = 0.3 * np.sin(2 * np.pi * c4_freq * t)\n",
    "e_note = 0.3 * np.sin(2 * np.pi * e4_freq * t)\n",
    "g_note = 0.3 * np.sin(2 * np.pi * g4_freq * t)\n",
    "\n",
    "# 화음 생성 (세 음을 합침)\n",
    "c_major_chord = c_note + e_note + g_note\n",
    "\n",
    "# 엔벨로프 적용 (페이드 인/아웃)\n",
    "envelope = np.ones_like(t)\n",
    "fade_samples = int(0.1 * sample_rate)  # 0.1초 페이드\n",
    "envelope[:fade_samples] = np.linspace(0, 1, fade_samples)\n",
    "envelope[-fade_samples:] = np.linspace(1, 0, fade_samples)\n",
    "\n",
    "c_major_chord_envelope = c_major_chord * envelope\n",
    "\n",
    "print(\"C 메이저 코드 생성 완료\")\n",
    "print(f\"주파수 구성: C4({c4_freq:.2f}Hz), E4({e4_freq:.2f}Hz), G4({g4_freq:.2f}Hz)\")\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# 처음 0.02초만 표시 (파형 구조 확인)\n",
    "plot_samples = int(0.02 * sample_rate)\n",
    "\n",
    "axes[0].plot(t[:plot_samples], c_major_chord[:plot_samples], 'b-', alpha=0.7)\n",
    "axes[0].set_title('C 메이저 코드 파형 (처음 20ms)')\n",
    "axes[0].set_xlabel('시간 (초)')\n",
    "axes[0].set_ylabel('진폭')\n",
    "\n",
    "axes[1].plot(t, c_major_chord_envelope, 'g-', alpha=0.7)\n",
    "axes[1].set_title('엔벨로프가 적용된 C 메이저 코드 (전체)')\n",
    "axes[1].set_xlabel('시간 (초)')\n",
    "axes[1].set_ylabel('진폭')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 오디오 파일 저장과 읽기\n",
    "\n",
    "### 3.1 오디오 파일 저장\n",
    "\n",
    "생성한 오디오 신호를 파일로 저장하면 다른 프로그램에서도 재생할 수 있다.\n",
    "WAV 형식은 무손실 압축으로 원본 품질을 유지한다.\n",
    "soundfile 라이브러리를 사용하면 다양한 오디오 형식을 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시 오디오 파일 저장\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# 임시 디렉토리 생성\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "audio_file_path = os.path.join(temp_dir, 'test_audio.wav')\n",
    "\n",
    "# 오디오 파일 저장\n",
    "sf.write(audio_file_path, c_major_chord_envelope, sample_rate)\n",
    "print(f\"오디오 파일 저장: {audio_file_path}\")\n",
    "\n",
    "# 파일 정보 확인\n",
    "file_info = sf.info(audio_file_path)\n",
    "print(f\"\\n파일 정보:\")\n",
    "print(f\"  - 샘플링 레이트: {file_info.samplerate} Hz\")\n",
    "print(f\"  - 채널 수: {file_info.channels}\")\n",
    "print(f\"  - 프레임 수: {file_info.frames}\")\n",
    "print(f\"  - 길이: {file_info.duration:.2f} 초\")\n",
    "print(f\"  - 형식: {file_info.format}\")\n",
    "print(f\"  - 서브타입: {file_info.subtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Librosa로 오디오 파일 읽기\n",
    "\n",
    "Librosa는 음악 정보 검색(MIR) 연구를 위한 파이썬 라이브러리다.\n",
    "오디오 파일을 자동으로 모노로 변환하고 정규화한다.\n",
    "다양한 오디오 분석 기능을 제공하여 음악 연구에 필수적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librosa로 오디오 파일 읽기\n",
    "y, sr = librosa.load(audio_file_path, sr=None)  # sr=None은 원본 샘플링 레이트 유지\n",
    "\n",
    "print(f\"Librosa로 읽은 오디오 정보:\")\n",
    "print(f\"  - 신호 형태: {y.shape}\")\n",
    "print(f\"  - 샘플링 레이트: {sr} Hz\")\n",
    "print(f\"  - 길이: {len(y) / sr:.2f} 초\")\n",
    "print(f\"  - 신호 범위: [{y.min():.3f}, {y.max():.3f}]\")\n",
    "print(f\"  - RMS 에너지: {np.sqrt(np.mean(y**2)):.3f}\")\n",
    "\n",
    "# 스테레오 파일 읽기 (mono=False 옵션)\n",
    "# 테스트를 위해 스테레오 파일 생성\n",
    "stereo_signal = np.vstack([c_major_chord_envelope, sine_wave[:len(c_major_chord_envelope)]])\n",
    "stereo_file_path = os.path.join(temp_dir, 'test_stereo.wav')\n",
    "sf.write(stereo_file_path, stereo_signal.T, sample_rate)\n",
    "\n",
    "y_stereo, sr_stereo = librosa.load(stereo_file_path, sr=None, mono=False)\n",
    "print(f\"\\n스테레오 오디오 정보:\")\n",
    "print(f\"  - 신호 형태: {y_stereo.shape}\")\n",
    "print(f\"  - 채널 수: {y_stereo.shape[0] if y_stereo.ndim > 1 else 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 시간 도메인 분석\n",
    "\n",
    "### 4.1 파형 시각화와 기본 특성\n",
    "\n",
    "시간 도메인에서는 신호의 진폭이 시간에 따라 어떻게 변화하는지 관찰한다.\n",
    "파형을 통해 음량 변화, 리듬 패턴, 무음 구간 등을 파악할 수 있다.\n",
    "오디오 편집과 기본적인 신호 처리는 주로 시간 도메인에서 수행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더 복잡한 테스트 신호 생성 (드럼 비트 시뮬레이션)\n",
    "beat_duration = 4.0\n",
    "beat_sr = 22050\n",
    "t_beat = np.linspace(0, beat_duration, int(beat_sr * beat_duration))\n",
    "\n",
    "# 킥 드럼 시뮬레이션 (낮은 주파수 + 빠른 감쇠)\n",
    "kick_times = [0, 1, 2, 3]  # 매 초마다 킥\n",
    "drum_signal = np.zeros_like(t_beat)\n",
    "\n",
    "for kick_time in kick_times:\n",
    "    kick_start = int(kick_time * beat_sr)\n",
    "    kick_duration = 0.2\n",
    "    kick_samples = int(kick_duration * beat_sr)\n",
    "    \n",
    "    if kick_start + kick_samples < len(drum_signal):\n",
    "        t_kick = np.linspace(0, kick_duration, kick_samples)\n",
    "        kick = np.sin(2 * np.pi * 60 * t_kick) * np.exp(-10 * t_kick)\n",
    "        drum_signal[kick_start:kick_start + kick_samples] += kick\n",
    "\n",
    "# 하이햇 추가 (높은 주파수 노이즈)\n",
    "hihat_times = np.arange(0, beat_duration, 0.25)  # 16분음표\n",
    "for hihat_time in hihat_times:\n",
    "    hihat_start = int(hihat_time * beat_sr)\n",
    "    hihat_duration = 0.05\n",
    "    hihat_samples = int(hihat_duration * beat_sr)\n",
    "    \n",
    "    if hihat_start + hihat_samples < len(drum_signal):\n",
    "        hihat = np.random.randn(hihat_samples) * 0.1 * np.exp(-20 * np.linspace(0, hihat_duration, hihat_samples))\n",
    "        drum_signal[hihat_start:hihat_start + hihat_samples] += hihat\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# 전체 파형\n",
    "axes[0].plot(t_beat, drum_signal, 'b-', linewidth=0.5)\n",
    "axes[0].set_title('드럼 비트 패턴 (전체)')\n",
    "axes[0].set_xlabel('시간 (초)')\n",
    "axes[0].set_ylabel('진폭')\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "# 첫 1초 확대\n",
    "zoom_samples = int(1.0 * beat_sr)\n",
    "axes[1].plot(t_beat[:zoom_samples], drum_signal[:zoom_samples], 'g-', linewidth=0.5)\n",
    "axes[1].set_title('드럼 비트 패턴 (첫 1초 확대)')\n",
    "axes[1].set_xlabel('시간 (초)')\n",
    "axes[1].set_ylabel('진폭')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 기본 통계\n",
    "print(f\"신호 통계:\")\n",
    "print(f\"  - Peak amplitude: {np.max(np.abs(drum_signal)):.3f}\")\n",
    "print(f\"  - RMS: {np.sqrt(np.mean(drum_signal**2)):.3f}\")\n",
    "print(f\"  - Zero crossing rate: {librosa.feature.zero_crossing_rate(drum_signal)[0].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 시간 도메인 특징 추출\n",
    "\n",
    "시간 도메인에서 추출할 수 있는 주요 특징들은 신호의 에너지 변화와 리듬 정보를 담고 있다.\n",
    "RMS 에너지는 음량의 변화를, Zero Crossing Rate는 신호의 주파수 특성을 간접적으로 나타낸다.\n",
    "이러한 특징들은 음악 장르 분류, 비트 검출 등에 활용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS 에너지 계산\n",
    "hop_length = 512\n",
    "frame_length = 2048\n",
    "\n",
    "rms = librosa.feature.rms(y=drum_signal, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "rms_time = librosa.frames_to_time(np.arange(len(rms)), sr=beat_sr, hop_length=hop_length)\n",
    "\n",
    "# Zero Crossing Rate\n",
    "zcr = librosa.feature.zero_crossing_rate(drum_signal, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "\n",
    "# Spectral Centroid (주파수 중심)\n",
    "spectral_centroid = librosa.feature.spectral_centroid(y=drum_signal, sr=beat_sr, hop_length=hop_length)[0]\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "\n",
    "# 원본 신호\n",
    "axes[0].plot(t_beat, drum_signal, 'b-', linewidth=0.5, alpha=0.7)\n",
    "axes[0].set_title('원본 신호')\n",
    "axes[0].set_ylabel('진폭')\n",
    "\n",
    "# RMS 에너지\n",
    "axes[1].plot(rms_time, rms, 'r-', linewidth=2)\n",
    "axes[1].set_title('RMS 에너지')\n",
    "axes[1].set_ylabel('RMS')\n",
    "axes[1].fill_between(rms_time, 0, rms, alpha=0.3)\n",
    "\n",
    "# Zero Crossing Rate\n",
    "axes[2].plot(rms_time, zcr, 'g-', linewidth=2)\n",
    "axes[2].set_title('Zero Crossing Rate')\n",
    "axes[2].set_ylabel('ZCR')\n",
    "\n",
    "# Spectral Centroid\n",
    "axes[3].plot(rms_time, spectral_centroid, 'm-', linewidth=2)\n",
    "axes[3].set_title('Spectral Centroid')\n",
    "axes[3].set_xlabel('시간 (초)')\n",
    "axes[3].set_ylabel('Hz')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"특징 통계:\")\n",
    "print(f\"  - 평균 RMS: {rms.mean():.4f}\")\n",
    "print(f\"  - 평균 ZCR: {zcr.mean():.4f}\")\n",
    "print(f\"  - 평균 Spectral Centroid: {spectral_centroid.mean():.2f} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 주파수 도메인 분석 (FFT)\n",
    "\n",
    "### 5.1 푸리에 변환 기초\n",
    "\n",
    "푸리에 변환(Fourier Transform)은 시간 도메인 신호를 주파수 도메인으로 변환한다.\n",
    "모든 복잡한 신호는 여러 사인파의 합으로 표현될 수 있다는 원리에 기반한다.\n",
    "FFT(Fast Fourier Transform)는 이산 푸리에 변환을 효율적으로 계산하는 알고리즘이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 신호 생성 (여러 주파수 성분)\n",
    "test_duration = 1.0\n",
    "test_sr = 8000  # 낮은 샘플링 레이트로 시각화 단순화\n",
    "t_test = np.linspace(0, test_duration, int(test_sr * test_duration))\n",
    "\n",
    "# 여러 주파수 성분을 가진 신호\n",
    "freq_components = [\n",
    "    (100, 1.0),   # 100Hz, 진폭 1.0\n",
    "    (250, 0.5),   # 250Hz, 진폭 0.5\n",
    "    (500, 0.3),   # 500Hz, 진폭 0.3\n",
    "    (1000, 0.2)   # 1000Hz, 진폭 0.2\n",
    "]\n",
    "\n",
    "complex_signal = np.zeros_like(t_test)\n",
    "for freq, amp in freq_components:\n",
    "    complex_signal += amp * np.sin(2 * np.pi * freq * t_test)\n",
    "\n",
    "# FFT 수행\n",
    "n_fft = len(complex_signal)\n",
    "fft_result = fft(complex_signal)\n",
    "fft_freq = fftfreq(n_fft, 1/test_sr)[:n_fft//2]  # 양의 주파수만\n",
    "fft_magnitude = np.abs(fft_result)[:n_fft//2] * 2/n_fft  # 정규화\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# 시간 도메인\n",
    "axes[0].plot(t_test[:200], complex_signal[:200], 'b-')\n",
    "axes[0].set_title('시간 도메인 신호 (처음 200 샘플)')\n",
    "axes[0].set_xlabel('시간 (초)')\n",
    "axes[0].set_ylabel('진폭')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 주파수 도메인\n",
    "axes[1].plot(fft_freq, fft_magnitude, 'r-', linewidth=2)\n",
    "axes[1].set_title('주파수 도메인 (FFT 결과)')\n",
    "axes[1].set_xlabel('주파수 (Hz)')\n",
    "axes[1].set_ylabel('크기')\n",
    "axes[1].set_xlim([0, 1500])\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 피크 주파수 표시\n",
    "for freq, amp in freq_components:\n",
    "    axes[1].axvline(x=freq, color='g', linestyle='--', alpha=0.5)\n",
    "    axes[1].text(freq, amp*0.9, f'{freq}Hz', rotation=90, va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 피크 주파수 검출\n",
    "peak_indices = signal.find_peaks(fft_magnitude, height=0.1)[0]\n",
    "peak_frequencies = fft_freq[peak_indices]\n",
    "peak_magnitudes = fft_magnitude[peak_indices]\n",
    "\n",
    "print(\"검출된 주파수 성분:\")\n",
    "for freq, mag in zip(peak_frequencies, peak_magnitudes):\n",
    "    print(f\"  - {freq:.1f} Hz: 크기 {mag:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 STFT (Short-Time Fourier Transform)\n",
    "\n",
    "STFT는 신호를 작은 시간 구간으로 나누어 각각에 FFT를 적용한다.\n",
    "시간에 따른 주파수 변화를 관찰할 수 있어 음악 분석에 필수적이다.\n",
    "스펙트로그램은 STFT 결과를 시각화한 것으로, 시간-주파수 표현을 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간에 따라 변하는 신호 생성 (주파수 스윕)\n",
    "sweep_duration = 3.0\n",
    "sweep_sr = 22050\n",
    "t_sweep = np.linspace(0, sweep_duration, int(sweep_sr * sweep_duration))\n",
    "\n",
    "# Linear chirp (주파수가 선형적으로 증가)\n",
    "f_start = 100\n",
    "f_end = 2000\n",
    "chirp_signal = signal.chirp(t_sweep, f_start, sweep_duration, f_end, method='linear')\n",
    "\n",
    "# STFT 계산\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "D = librosa.stft(chirp_signal, n_fft=n_fft, hop_length=hop_length)\n",
    "D_magnitude = np.abs(D)\n",
    "D_db = librosa.amplitude_to_db(D_magnitude, ref=np.max)\n",
    "\n",
    "# 스펙트로그램 시각화\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# 원본 신호\n",
    "axes[0].plot(t_sweep, chirp_signal, 'b-', linewidth=0.5)\n",
    "axes[0].set_title('Chirp 신호 (주파수 스윕)')\n",
    "axes[0].set_ylabel('진폭')\n",
    "axes[0].set_xlim([0, sweep_duration])\n",
    "\n",
    "# 선형 스케일 스펙트로그램\n",
    "img1 = librosa.display.specshow(D_magnitude, y_axis='linear', x_axis='time',\n",
    "                                sr=sweep_sr, hop_length=hop_length, ax=axes[1])\n",
    "axes[1].set_title('스펙트로그램 (선형 스케일)')\n",
    "axes[1].set_ylabel('주파수 (Hz)')\n",
    "fig.colorbar(img1, ax=axes[1], format='%+2.0f')\n",
    "\n",
    "# 로그 스케일 스펙트로그램 (dB)\n",
    "img2 = librosa.display.specshow(D_db, y_axis='log', x_axis='time',\n",
    "                                sr=sweep_sr, hop_length=hop_length, ax=axes[2])\n",
    "axes[2].set_title('스펙트로그램 (로그 스케일, dB)')\n",
    "axes[2].set_xlabel('시간 (초)')\n",
    "axes[2].set_ylabel('주파수 (Hz)')\n",
    "fig.colorbar(img2, ax=axes[2], format='%+2.0f dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"STFT 결과:\")\n",
    "print(f\"  - STFT 행렬 크기: {D.shape}\")\n",
    "print(f\"  - 주파수 빈 수: {D.shape[0]}\")\n",
    "print(f\"  - 시간 프레임 수: {D.shape[1]}\")\n",
    "print(f\"  - 주파수 해상도: {sweep_sr/n_fft:.2f} Hz/bin\")\n",
    "print(f\"  - 시간 해상도: {hop_length/sweep_sr*1000:.2f} ms/frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 오디오 특징 추출\n",
    "\n",
    "### 6.1 MFCC (Mel-frequency Cepstral Coefficients)\n",
    "\n",
    "MFCC는 음성 인식과 음악 정보 검색에서 가장 널리 사용되는 특징이다.\n",
    "인간의 청각 시스템을 모방한 Mel 스케일을 사용하여 주파수를 변환한다.\n",
    "음색(timbre) 정보를 효과적으로 표현하여 악기 분류, 장르 분류 등에 활용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음악적 신호 생성 (여러 악기 시뮬레이션)\n",
    "music_duration = 2.0\n",
    "music_sr = 22050\n",
    "t_music = np.linspace(0, music_duration, int(music_sr * music_duration))\n",
    "\n",
    "# 피아노 음 시뮬레이션 (기본음 + 하모닉스)\n",
    "fundamental = 261.63  # C4\n",
    "piano_signal = np.zeros_like(t_music)\n",
    "harmonics = [1, 2, 3, 4, 5]  # 기본음과 배음들\n",
    "harmonic_amplitudes = [1.0, 0.5, 0.3, 0.2, 0.1]\n",
    "\n",
    "for h, amp in zip(harmonics, harmonic_amplitudes):\n",
    "    piano_signal += amp * np.sin(2 * np.pi * fundamental * h * t_music)\n",
    "\n",
    "# ADSR 엔벨로프 적용\n",
    "envelope = np.ones_like(t_music)\n",
    "attack_time = 0.01\n",
    "decay_time = 0.1\n",
    "sustain_level = 0.7\n",
    "release_time = 0.3\n",
    "\n",
    "attack_samples = int(attack_time * music_sr)\n",
    "decay_samples = int(decay_time * music_sr)\n",
    "release_samples = int(release_time * music_sr)\n",
    "\n",
    "envelope[:attack_samples] = np.linspace(0, 1, attack_samples)\n",
    "envelope[attack_samples:attack_samples+decay_samples] = np.linspace(1, sustain_level, decay_samples)\n",
    "envelope[attack_samples+decay_samples:-release_samples] = sustain_level\n",
    "envelope[-release_samples:] = np.linspace(sustain_level, 0, release_samples)\n",
    "\n",
    "piano_signal = piano_signal * envelope\n",
    "\n",
    "# MFCC 추출\n",
    "n_mfcc = 13\n",
    "mfccs = librosa.feature.mfcc(y=piano_signal, sr=music_sr, n_mfcc=n_mfcc)\n",
    "\n",
    "# Delta MFCC (1차 미분)\n",
    "mfccs_delta = librosa.feature.delta(mfccs)\n",
    "\n",
    "# Delta-Delta MFCC (2차 미분)\n",
    "mfccs_delta2 = librosa.feature.delta(mfccs, order=2)\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "\n",
    "# 원본 신호\n",
    "axes[0].plot(t_music[:1000], piano_signal[:1000], 'b-', linewidth=0.5)\n",
    "axes[0].set_title('피아노 신호 (처음 1000 샘플)')\n",
    "axes[0].set_ylabel('진폭')\n",
    "\n",
    "# MFCC\n",
    "img1 = librosa.display.specshow(mfccs, x_axis='time', sr=music_sr, ax=axes[1])\n",
    "axes[1].set_title('MFCC')\n",
    "axes[1].set_ylabel('MFCC 계수')\n",
    "fig.colorbar(img1, ax=axes[1])\n",
    "\n",
    "# Delta MFCC\n",
    "img2 = librosa.display.specshow(mfccs_delta, x_axis='time', sr=music_sr, ax=axes[2])\n",
    "axes[2].set_title('Delta MFCC (1차 미분)')\n",
    "axes[2].set_ylabel('MFCC 계수')\n",
    "fig.colorbar(img2, ax=axes[2])\n",
    "\n",
    "# Delta-Delta MFCC\n",
    "img3 = librosa.display.specshow(mfccs_delta2, x_axis='time', sr=music_sr, ax=axes[3])\n",
    "axes[3].set_title('Delta-Delta MFCC (2차 미분)')\n",
    "axes[3].set_xlabel('시간 (초)')\n",
    "axes[3].set_ylabel('MFCC 계수')\n",
    "fig.colorbar(img3, ax=axes[3])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"MFCC 특징:\")\n",
    "print(f\"  - MFCC 차원: {mfccs.shape}\")\n",
    "print(f\"  - 계수 개수: {n_mfcc}\")\n",
    "print(f\"  - 시간 프레임 수: {mfccs.shape[1]}\")\n",
    "print(f\"  - 평균 MFCC (각 계수별):\")\n",
    "for i in range(min(5, n_mfcc)):\n",
    "    print(f\"    MFCC[{i}]: {mfccs[i].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 스펙트럴 특징 (Spectral Features)\n",
    "\n",
    "스펙트럴 특징들은 주파수 도메인에서 추출되는 다양한 통계적 특성이다.\n",
    "Spectral Centroid는 음색의 밝기를, Spectral Rolloff는 주파수 분포를 나타낸다.\n",
    "이러한 특징들은 악기 인식, 음악 분류, 음질 평가 등에 활용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 스펙트럴 특징 추출\n",
    "# Spectral Centroid - 주파수 분포의 중심\n",
    "spec_centroid = librosa.feature.spectral_centroid(y=piano_signal, sr=music_sr)[0]\n",
    "\n",
    "# Spectral Bandwidth - 주파수 분포의 폭\n",
    "spec_bandwidth = librosa.feature.spectral_bandwidth(y=piano_signal, sr=music_sr)[0]\n",
    "\n",
    "# Spectral Rolloff - 에너지의 85%가 포함되는 주파수\n",
    "spec_rolloff = librosa.feature.spectral_rolloff(y=piano_signal, sr=music_sr, roll_percent=0.85)[0]\n",
    "\n",
    "# Spectral Contrast - 주파수 대역별 대비\n",
    "spec_contrast = librosa.feature.spectral_contrast(y=piano_signal, sr=music_sr)\n",
    "\n",
    "# Zero Crossing Rate\n",
    "zcr = librosa.feature.zero_crossing_rate(piano_signal)[0]\n",
    "\n",
    "# 시간 축 생성\n",
    "frames = range(len(spec_centroid))\n",
    "t_frames = librosa.frames_to_time(frames, sr=music_sr)\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 12))\n",
    "\n",
    "# Spectral Centroid\n",
    "axes[0].plot(t_frames, spec_centroid, 'b-', linewidth=2)\n",
    "axes[0].set_title('Spectral Centroid')\n",
    "axes[0].set_ylabel('Hz')\n",
    "axes[0].fill_between(t_frames, 0, spec_centroid, alpha=0.3)\n",
    "\n",
    "# Spectral Bandwidth\n",
    "axes[1].plot(t_frames, spec_bandwidth, 'r-', linewidth=2)\n",
    "axes[1].set_title('Spectral Bandwidth')\n",
    "axes[1].set_ylabel('Hz')\n",
    "axes[1].fill_between(t_frames, 0, spec_bandwidth, alpha=0.3)\n",
    "\n",
    "# Spectral Rolloff\n",
    "axes[2].plot(t_frames, spec_rolloff, 'g-', linewidth=2)\n",
    "axes[2].set_title('Spectral Rolloff (85%)')\n",
    "axes[2].set_ylabel('Hz')\n",
    "axes[2].fill_between(t_frames, 0, spec_rolloff, alpha=0.3)\n",
    "\n",
    "# Spectral Contrast\n",
    "img = librosa.display.specshow(spec_contrast, x_axis='time', ax=axes[3])\n",
    "axes[3].set_title('Spectral Contrast')\n",
    "axes[3].set_ylabel('Frequency bands')\n",
    "fig.colorbar(img, ax=axes[3])\n",
    "\n",
    "# Zero Crossing Rate\n",
    "axes[4].plot(t_frames, zcr, 'm-', linewidth=2)\n",
    "axes[4].set_title('Zero Crossing Rate')\n",
    "axes[4].set_xlabel('시간 (초)')\n",
    "axes[4].set_ylabel('Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 특징 통계\n",
    "print(\"스펙트럴 특징 통계:\")\n",
    "print(f\"  - Spectral Centroid: {spec_centroid.mean():.2f} ± {spec_centroid.std():.2f} Hz\")\n",
    "print(f\"  - Spectral Bandwidth: {spec_bandwidth.mean():.2f} ± {spec_bandwidth.std():.2f} Hz\")\n",
    "print(f\"  - Spectral Rolloff: {spec_rolloff.mean():.2f} ± {spec_rolloff.std():.2f} Hz\")\n",
    "print(f\"  - Zero Crossing Rate: {zcr.mean():.4f} ± {zcr.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 고급 스펙트로그램 분석\n",
    "\n",
    "### 7.1 Mel Spectrogram\n",
    "\n",
    "Mel 스펙트로그램은 인간의 청각 특성을 반영한 주파수 표현이다.\n",
    "저주파에서는 해상도가 높고, 고주파로 갈수록 해상도가 낮아진다.\n",
    "음악 분류, 음성 인식 등 다양한 응용 분야에서 표준적으로 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복잡한 음악 신호 생성 (여러 음과 리듬)\n",
    "complex_duration = 4.0\n",
    "complex_sr = 22050\n",
    "t_complex = np.linspace(0, complex_duration, int(complex_sr * complex_duration))\n",
    "\n",
    "# 멜로디 라인 생성\n",
    "melody_notes = [261.63, 293.66, 329.63, 349.23, 392.00, 349.23, 329.63, 293.66]  # C-D-E-F-G-F-E-D\n",
    "note_duration = 0.5\n",
    "melody_signal = np.zeros_like(t_complex)\n",
    "\n",
    "for i, note_freq in enumerate(melody_notes):\n",
    "    start_idx = int(i * note_duration * complex_sr)\n",
    "    end_idx = int((i + 1) * note_duration * complex_sr)\n",
    "    if end_idx <= len(melody_signal):\n",
    "        t_note = np.linspace(0, note_duration, end_idx - start_idx)\n",
    "        note_envelope = np.exp(-3 * t_note)  # 감쇠 엔벨로프\n",
    "        melody_signal[start_idx:end_idx] = note_envelope * np.sin(2 * np.pi * note_freq * t_note)\n",
    "\n",
    "# 베이스 라인 추가\n",
    "bass_freq = 65.41  # C2\n",
    "bass_signal = 0.3 * np.sin(2 * np.pi * bass_freq * t_complex)\n",
    "\n",
    "# 전체 믹스\n",
    "mixed_signal = melody_signal + bass_signal\n",
    "\n",
    "# Mel spectrogram 계산\n",
    "n_mels = 128\n",
    "mel_spec = librosa.feature.melspectrogram(y=mixed_signal, sr=complex_sr, n_mels=n_mels)\n",
    "mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "# 일반 스펙트로그램과 비교\n",
    "D_linear = librosa.stft(mixed_signal)\n",
    "D_linear_db = librosa.amplitude_to_db(np.abs(D_linear), ref=np.max)\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# 원본 신호\n",
    "axes[0].plot(t_complex, mixed_signal, 'b-', linewidth=0.5)\n",
    "axes[0].set_title('음악 신호 (멜로디 + 베이스)')\n",
    "axes[0].set_ylabel('진폭')\n",
    "axes[0].set_xlim([0, complex_duration])\n",
    "\n",
    "# 일반 스펙트로그램\n",
    "img1 = librosa.display.specshow(D_linear_db, y_axis='linear', x_axis='time',\n",
    "                                sr=complex_sr, ax=axes[1])\n",
    "axes[1].set_title('Linear Frequency Spectrogram')\n",
    "axes[1].set_ylabel('주파수 (Hz)')\n",
    "fig.colorbar(img1, ax=axes[1], format='%+2.0f dB')\n",
    "\n",
    "# Mel 스펙트로그램\n",
    "img2 = librosa.display.specshow(mel_spec_db, y_axis='mel', x_axis='time',\n",
    "                                sr=complex_sr, ax=axes[2])\n",
    "axes[2].set_title('Mel Spectrogram')\n",
    "axes[2].set_xlabel('시간 (초)')\n",
    "axes[2].set_ylabel('Mel 주파수')\n",
    "fig.colorbar(img2, ax=axes[2], format='%+2.0f dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mel Spectrogram 정보:\")\n",
    "print(f\"  - 형태: {mel_spec.shape}\")\n",
    "print(f\"  - Mel 밴드 수: {n_mels}\")\n",
    "print(f\"  - 시간 프레임 수: {mel_spec.shape[1]}\")\n",
    "print(f\"  - 최소값: {mel_spec_db.min():.2f} dB\")\n",
    "print(f\"  - 최대값: {mel_spec_db.max():.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Chroma Features\n",
    "\n",
    "Chroma 특징은 음악의 화성 정보를 12개의 반음계로 표현한다.\n",
    "옥타브와 무관하게 같은 음(C, C#, D, ...)을 하나의 클래스로 매핑한다.\n",
    "코드 인식, 키 검출, 음악 유사도 측정 등에 활용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma 특징 추출\n",
    "chroma_stft = librosa.feature.chroma_stft(y=mixed_signal, sr=complex_sr)\n",
    "chroma_cqt = librosa.feature.chroma_cqt(y=mixed_signal, sr=complex_sr)\n",
    "chroma_cens = librosa.feature.chroma_cens(y=mixed_signal, sr=complex_sr)\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 9))\n",
    "\n",
    "# Chroma STFT\n",
    "img1 = librosa.display.specshow(chroma_stft, y_axis='chroma', x_axis='time',\n",
    "                                sr=complex_sr, ax=axes[0])\n",
    "axes[0].set_title('Chromagram (STFT-based)')\n",
    "axes[0].set_ylabel('Pitch Class')\n",
    "fig.colorbar(img1, ax=axes[0])\n",
    "\n",
    "# Chroma CQT\n",
    "img2 = librosa.display.specshow(chroma_cqt, y_axis='chroma', x_axis='time',\n",
    "                                sr=complex_sr, ax=axes[1])\n",
    "axes[1].set_title('Chromagram (CQT-based)')\n",
    "axes[1].set_ylabel('Pitch Class')\n",
    "fig.colorbar(img2, ax=axes[1])\n",
    "\n",
    "# Chroma CENS\n",
    "img3 = librosa.display.specshow(chroma_cens, y_axis='chroma', x_axis='time',\n",
    "                                sr=complex_sr, ax=axes[2])\n",
    "axes[2].set_title('Chromagram (CENS)')\n",
    "axes[2].set_xlabel('시간 (초)')\n",
    "axes[2].set_ylabel('Pitch Class')\n",
    "fig.colorbar(img3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 평균 Chroma 벡터 (전체 곡의 키 추정에 사용)\n",
    "mean_chroma = chroma_stft.mean(axis=1)\n",
    "pitch_classes = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n",
    "\n",
    "print(\"평균 Chroma 벡터:\")\n",
    "for pc, val in zip(pitch_classes, mean_chroma):\n",
    "    print(f\"  {pc:2s}: {'█' * int(val * 20):<20s} {val:.3f}\")\n",
    "\n",
    "# 추정된 키\n",
    "estimated_key_idx = np.argmax(mean_chroma)\n",
    "print(f\"\\n추정된 주요 음: {pitch_classes[estimated_key_idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 실전 응용: 음원 특징 추출 파이프라인\n",
    "\n",
    "### 8.1 종합 특징 추출 함수\n",
    "\n",
    "실제 음원 분석 프로젝트에서는 여러 특징을 동시에 추출하여 사용한다.\n",
    "각 특징은 음원의 다른 측면을 표현하므로, 조합하면 더 정확한 분석이 가능하다.\n",
    "추출된 특징들은 머신러닝 모델의 입력으로 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(y, sr, n_mfcc=13, n_mels=128):\n",
    "    \"\"\"\n",
    "    오디오 신호에서 다양한 특징을 추출하는 함수\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : np.ndarray\n",
    "        오디오 신호\n",
    "    sr : int\n",
    "        샘플링 레이트\n",
    "    n_mfcc : int\n",
    "        MFCC 계수 개수\n",
    "    n_mels : int\n",
    "        Mel 밴드 수\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    features : dict\n",
    "        추출된 특징들\n",
    "    \"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    # 시간 도메인 특징\n",
    "    features['rms_mean'] = float(librosa.feature.rms(y=y).mean())\n",
    "    features['rms_std'] = float(librosa.feature.rms(y=y).std())\n",
    "    features['zcr_mean'] = float(librosa.feature.zero_crossing_rate(y).mean())\n",
    "    features['zcr_std'] = float(librosa.feature.zero_crossing_rate(y).std())\n",
    "    \n",
    "    # 스펙트럴 특징\n",
    "    spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    features['spectral_centroid_mean'] = float(spec_cent.mean())\n",
    "    features['spectral_centroid_std'] = float(spec_cent.std())\n",
    "    \n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    features['spectral_bandwidth_mean'] = float(spec_bw.mean())\n",
    "    features['spectral_bandwidth_std'] = float(spec_bw.std())\n",
    "    \n",
    "    spec_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    features['spectral_rolloff_mean'] = float(spec_rolloff.mean())\n",
    "    features['spectral_rolloff_std'] = float(spec_rolloff.std())\n",
    "    \n",
    "    # MFCC\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    for i in range(n_mfcc):\n",
    "        features[f'mfcc_{i}_mean'] = float(mfccs[i].mean())\n",
    "        features[f'mfcc_{i}_std'] = float(mfccs[i].std())\n",
    "    \n",
    "    # Chroma\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    for i in range(12):\n",
    "        features[f'chroma_{i}_mean'] = float(chroma[i].mean())\n",
    "    \n",
    "    # Tempo\n",
    "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    features['tempo'] = float(tempo)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# 테스트\n",
    "features = extract_audio_features(mixed_signal, complex_sr)\n",
    "\n",
    "# 특징 출력\n",
    "print(\"추출된 오디오 특징:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 카테고리별로 정리\n",
    "time_features = {k: v for k, v in features.items() if 'rms' in k or 'zcr' in k}\n",
    "spectral_features = {k: v for k, v in features.items() if 'spectral' in k}\n",
    "mfcc_features = {k: v for k, v in features.items() if 'mfcc' in k}\n",
    "chroma_features = {k: v for k, v in features.items() if 'chroma' in k}\n",
    "rhythm_features = {k: v for k, v in features.items() if 'tempo' in k}\n",
    "\n",
    "print(\"\\n시간 도메인 특징:\")\n",
    "for k, v in time_features.items():\n",
    "    print(f\"  {k:25s}: {v:8.4f}\")\n",
    "\n",
    "print(\"\\n스펙트럴 특징:\")\n",
    "for k, v in spectral_features.items():\n",
    "    print(f\"  {k:25s}: {v:8.2f}\")\n",
    "\n",
    "print(\"\\n리듬 특징:\")\n",
    "for k, v in rhythm_features.items():\n",
    "    print(f\"  {k:25s}: {v:8.2f}\")\n",
    "\n",
    "print(f\"\\n총 특징 개수: {len(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 특징 벡터를 활용한 음원 비교\n",
    "\n",
    "추출된 특징 벡터를 사용하여 음원 간의 유사도를 계산할 수 있다.\n",
    "유클리드 거리, 코사인 유사도 등 다양한 거리 측정 방법을 사용한다.\n",
    "이는 음악 추천 시스템, 유사 음원 검색 등에 활용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 다른 신호 생성 및 특징 추출\n",
    "signals = {}\n",
    "all_features = {}\n",
    "\n",
    "# 신호 1: 낮은 주파수 사인파\n",
    "t_test = np.linspace(0, 1, complex_sr)\n",
    "signals['Low_Sine'] = 0.5 * np.sin(2 * np.pi * 100 * t_test)\n",
    "\n",
    "# 신호 2: 높은 주파수 사인파\n",
    "signals['High_Sine'] = 0.5 * np.sin(2 * np.pi * 1000 * t_test)\n",
    "\n",
    "# 신호 3: 화이트 노이즈\n",
    "signals['White_Noise'] = np.random.randn(complex_sr) * 0.1\n",
    "\n",
    "# 신호 4: 복합 신호 (여러 주파수)\n",
    "signals['Complex'] = (0.3 * np.sin(2 * np.pi * 200 * t_test) + \n",
    "                      0.2 * np.sin(2 * np.pi * 400 * t_test) +\n",
    "                      0.1 * np.sin(2 * np.pi * 800 * t_test))\n",
    "\n",
    "# 각 신호의 특징 추출\n",
    "for name, signal in signals.items():\n",
    "    all_features[name] = extract_audio_features(signal, complex_sr)\n",
    "\n",
    "# 특징 벡터로 변환 (선택된 특징만 사용)\n",
    "selected_features = ['rms_mean', 'zcr_mean', 'spectral_centroid_mean', \n",
    "                    'spectral_bandwidth_mean', 'mfcc_0_mean', 'mfcc_1_mean']\n",
    "\n",
    "feature_vectors = {}\n",
    "for name, features in all_features.items():\n",
    "    feature_vectors[name] = np.array([features[f] for f in selected_features])\n",
    "\n",
    "# 유사도 행렬 계산\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "\n",
    "signal_names = list(feature_vectors.keys())\n",
    "n_signals = len(signal_names)\n",
    "\n",
    "euclidean_distances = np.zeros((n_signals, n_signals))\n",
    "cosine_similarities = np.zeros((n_signals, n_signals))\n",
    "\n",
    "for i in range(n_signals):\n",
    "    for j in range(n_signals):\n",
    "        vec_i = feature_vectors[signal_names[i]]\n",
    "        vec_j = feature_vectors[signal_names[j]]\n",
    "        \n",
    "        euclidean_distances[i, j] = euclidean(vec_i, vec_j)\n",
    "        if i == j:\n",
    "            cosine_similarities[i, j] = 1.0\n",
    "        else:\n",
    "            cosine_similarities[i, j] = 1 - cosine(vec_i, vec_j)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"특징 벡터 비교:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"사용된 특징: {selected_features}\")\n",
    "print()\n",
    "\n",
    "# 특징 값 표시\n",
    "feature_df = pd.DataFrame(feature_vectors).T\n",
    "feature_df.columns = selected_features\n",
    "print(\"특징 값:\")\n",
    "print(feature_df.round(3))\n",
    "print()\n",
    "\n",
    "# 유클리드 거리\n",
    "distance_df = pd.DataFrame(euclidean_distances, \n",
    "                          index=signal_names, \n",
    "                          columns=signal_names)\n",
    "print(\"\\n유클리드 거리 행렬:\")\n",
    "print(distance_df.round(3))\n",
    "\n",
    "# 코사인 유사도\n",
    "similarity_df = pd.DataFrame(cosine_similarities,\n",
    "                            index=signal_names,\n",
    "                            columns=signal_names)\n",
    "print(\"\\n코사인 유사도 행렬:\")\n",
    "print(similarity_df.round(3))\n",
    "\n",
    "# 가장 유사한 신호 쌍 찾기\n",
    "np.fill_diagonal(euclidean_distances, np.inf)\n",
    "min_dist_idx = np.unravel_index(euclidean_distances.argmin(), euclidean_distances.shape)\n",
    "print(f\"\\n가장 유사한 신호 쌍: {signal_names[min_dist_idx[0]]} - {signal_names[min_dist_idx[1]]}\")\n",
    "print(f\"유클리드 거리: {euclidean_distances[min_dist_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 추천 실습\n",
    "\n",
    "### 실습 1: 자신만의 오디오 신호 생성\n",
    "\n",
    "다양한 악기 소리를 수학적으로 모델링해보자.\n",
    "기본음과 배음의 조합, ADSR 엔벨로프를 활용하여 실제 악기와 유사한 소리를 만들어보자.\n",
    "생성한 신호를 파일로 저장하고 실제로 들어보면서 차이를 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 제안:\n",
    "# 1. 플루트 소리 생성 (순수한 사인파 + 약간의 노이즈)\n",
    "# 2. 기타 소리 생성 (플럭 엔벨로프 + 풍부한 배음)\n",
    "# 3. 드럼 소리 생성 (짧은 버스트 + 노이즈)\n",
    "# 4. 신시사이저 소리 (다양한 파형의 조합)\n",
    "# 5. 각 소리의 스펙트로그램을 비교하여 특성 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습 2: 실제 음원 파일 분석\n",
    "\n",
    "자신이 좋아하는 음악 파일을 분석해보자.\n",
    "다양한 특징을 추출하고 시각화하여 음악의 구조를 이해해보자.\n",
    "여러 장르의 음악을 비교하여 각 장르의 특성을 파악해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실습 제안:\n",
    "# 1. MP3/WAV 파일을 librosa로 로드\n",
    "# 2. 전체 곡의 스펙트로그램 생성\n",
    "# 3. 구간별 에너지 변화 분석 (intro, verse, chorus 구분)\n",
    "# 4. 템포와 비트 검출\n",
    "# 5. 주요 음정(key) 추정\n",
    "# 6. 특징 벡터를 추출하여 유사한 곡 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 심화 학습을 위한 제안\n",
    "\n",
    "더 깊이 있는 학습을 원한다면 다음 주제들을 탐구해보자:\n",
    "\n",
    "1. **고급 신호처리 기법**: 웨이블릿 변환, 힐버트 변환, 적응 필터링\n",
    "2. **음원 분리**: NMF, ICA를 활용한 소스 분리\n",
    "3. **오디오 효과**: 리버브, 딜레이, 코러스 등 DSP 효과 구현\n",
    "4. **실시간 처리**: 스트리밍 오디오 처리, 레이턴시 최적화\n",
    "5. **딥러닝 응용**: CNN을 활용한 음원 분류, RNN을 활용한 음악 생성\n",
    "\n",
    "이러한 기술들은 음악 정보 검색, 오디오 엔지니어링, 음악 제작 등 다양한 분야에서 활용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시 파일 정리\n",
    "import shutil\n",
    "if 'temp_dir' in locals():\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(\"임시 파일 정리 완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}